\chapter{Discussion}

The computer scientist and mathematician Richard Hamming in his insightful essay \emph{You and Your Research} describes how he would pose the following question to his colleagues at Bell Labs -- \emph{`What is the most important question in your field, and why aren't you working on it?'}. As might be expected, this made him unpopular with many of his colleagues. However, it speaks an important truth -- the absolute and overriding importance of posing and working on the right questions. An important question that is impossible is futile. A tractable but unimportant question is useless. Throughout my PhD, I have endeavoured to orient by Hamming's maxim; to find the most important yet solvable question within my field and to try to answer it. This thesis, then, can be seen as a concatenation of three questions of progressively (in my opinion) increasing scope and importance.

The first question is local to the active inference community, but is very important within it. Namely, can active inference be combined with contemporary deep reinforcement learning methods, and thus be scaled to the kind of tasks that can be handled by contemporary deep reinforcement learning? Conversely, does the theory of active inference itself contain any insights which can be useful for machine learning theorists and practitioners? I believe that through my work \citep{millidge_deep_2019,millidge2019combining,tschantz2020reinforcement,millidge2020relationship,tschantz2020control} and others \citep{tschantz_scaling_2019,fountas2020deep,ueltzhoffer_deep_2018,ccatal2020learning}, both sides of this question have been definitively answered in the affirmative. Active inference can be straightforwardly scaled up using artificial neural networks and the techniques of deep reinforcement learning while, conversely, active inference has many interesting properties and ideas which could be of use to the deep reinforcement learning community. In this thesis, we have explored several of these ideas and primarily focused on how deep active inference and deep reinforcement learning can be merged. Now that this has been answered, future work should focus on the converse -- how deep active inference \emph{differs} from deep reinforcement learning and the extent to which it can inform and lead to novel and performant algorithms in deep reinforcement learning.  

The second question is more broadly targeted to the reinforcement learning and cognitive science communities, and concerns the \emph{mathematical origins of exploration}. This question is central to a number of related disciplines such as reinforcement learning \citep{sutton2018reinforcement}, decision theory \citep{daw2006cortical}, control theory \citep{kalman1960contributions}, and behavioural economics \citep{tversky1974judgment}, which all share the same fundamental object of study -- adaptive decision-making under uncertainty. Where there is uncertainty so that the true dynamics of the environment and/or the value of each possible contingency are not known, then the optimal policy cannot straightforwardly be computed, and agents are necessarily faced with the exploration-exploitation trade-off. This trade-off arises because new information can generally only be obtained by trying new courses of action or venturing into new regions of the state-space. However, to \emph{explore} new regions necessarily has an opportunity cost of not doing what you thought to be the current best option, which could instead have been \emph{exploited}. Given that to succeed at complex tasks, it is almost always necessary to explore, it is important to figure out how to explore in the most efficient manner. Specifically, we wish to design algorithms that can acquire the information necessary for success as rapidly as possible while incurring the minimum opportunity cost. In the literature, it has been discovered that a very good heuristic for doing this is simply to optimize a combination of the greedy reward maximization objective \emph{exploit} with an additional information gain exploration term \emph{explore} \citep{shyam_model-based_2019,schmidhuber2007simple,tschantz2020reinforcement}. Specifically, the reward maximization part of the objective ensures that agents do not spend large amounts of time exploring informative but barren regions, while the information gain terms help the agent not to get stuck in locally greedy, but globally poor optima. While this objective works well in practice, its mathematical origin and nature remains obscure. In the literature, this approach is often described intuitively as simply adding an additional exploratory term to the loss function. While random entropy-maximizing exploration can be derived straightforwardly from variational inference approaches to action \citep{levine2018reinforcement}, the mathematical origin and commitments of specifically optimizing information-seeking exploration terms has remained mysterious. This is the second question we set out to answer in this thesis -- mathematically, from what sort of fundamental objectives do information-gain exploration terms arise, and how can we characterise the possible space of such objectives?

In Chapter 5, we answer this question. We show that information gain maximizing exploration arises from minimizing the divergence between two distributions -- a predicted or expected distribution over likely states, given actions, and a desired distribution over states, which encodes the goals of the agent. This differs crucially from \emph{evidence} objectives, which are typically used in control as inference schemes \citep{levine2018reinforcement,rawlik2013stochastic}, which only seek to maximize the likelihood of the desired states, rather than explicitly match the two distributions. This finding has important implications for a wide range of fields. Specifically, we argue that any kind of information-maximizing exploratory behaviour can be seen as implicitly aiming for a matching of two distributions rather than a likelihood maximization. This, for instance, can explain several phenomenon, such as the probability matching behaviour that is regularly observed in human participants in cognitive science and behavioural economics tasks \citep{vulkan2000economist,daw2006cortical,west2003probability,shanks2002re}, which are puzzling under the presumption of evidence maximization \citep{tversky1974judgment,gaissmaier2008smart}. Moreover, by understanding the origin of information-seeking behaviour as emerging directly from divergence objectives, it provides us with a greater and deeper understanding of what agents which optimize these information-seeking terms are actually doing, while the ensuing mathematical understanding may allow us to manipulate these terms more confidently into more easily computable or tractable versions which could aid implementations directly.

The third question is one with the greatest scope and importance. This question is how can credit assignment be implemented in the brain? And, specifically, whether and how (if it does) the brain can implement the backpropagation of error algorithm. The solution to such a question would be of great importance to neuroscience, since it would provide a unifying view and mechanistic, algorithmic explanation of at least part of cortical function. Moreover, it would explain at a detailed level one of the core functionalities of the brain, and the one that underpins almost all adaptive behaviour. Credit assignment is crucial to any kind of long-range learning of the kind that \emph{must} be occurring in the brain. It is crucial for everything from learning the best way to form and interpret sensory representations, to action selection operations to, potentially long term memory and complex cognitive processing. While the brain undoubtedly performs a substantial amount of top-down contextual feedback processing as well as various kinds of homeostatic plasticity, which both remain poorly understood, we also know from the stunning success of machine learning in the last decade that simple feedforward passes on large neural networks trained with the backpropagation of error algorithm can accomplish tasks such as visual object recognition \citep{krizhevsky2012imagenet,child2020very}, generating realistic images from text inputs (\citep{radford2021learning}, human-passable natural language generation \citep{radford2019language}, and playing at a superhuman level games such as Go \citep{silver2017mastering}, Atari \citep{mnih2015human,mnih2013playing,schrittwieser2019mastering}, and Starcraft \citep{vinyals2019grandmaster}, which ten years ago were thought to be extremely challenging, if not impossible for computers to accomplish. Credit assignment, then, must be one of the core operations in the brain, and if we can understand this then it is possible we may obtain a grasp on how known computational algorithms are implemented in the brain which allows us to grapple tractably with its immense complexity. 

While I, and the field as a whole, have taken steps towards addressing and answering this question, we are still a long way from a viable solution for the brain. Nevertheless, I feel that, in general, with the profusion of algorithms addressing issues such as the weight transport problem \citep{lillicrap2016random,akrout2019deep,nokland2016direct}, and well as addressing issues of locality \citep{ororbia2019biologically,whittington2017approximation,scellier2016towards}, the field is close to a good solution for the case of rate-coded neurons on a temporally static graph. However, a solution under these constraints is fundamentally only an abstraction of a much messier reality, where neurons in the brain are not rate-coded but spiking, and must also achieve credit assignment not just across space (layers) but across time \citep{lillicrap2020backpropagation}. While there are some approaches which grapple with these additional, and harder problems \citep{zenke2018superspike,bellec2020solution}, there are not many and we are far from a viable global solution to this problem. I suspect it is into these new domains that future research should primarily be directed, and where important advances will be made. 

\section{Question 1: Scaling Active Inference}

It turns out the active inference approaches can be quite straightforwardly merged with those used in deep reinforcement learning and can thus straightforwardly be `scaled up' to achieve performance comparable with the state of the art. Moreover, different choices lead directly to different schools of model-free or model-based reinforcement learning. Specifically, active inference fundamentally operates on several core probabilistic distributions and objectives. The key distributions are the likelihood distribution $p(o | x)$, and the transition distribution $p(x_t | x_{t-1},a_t)$. While standard discrete-state-space active inference approaches parametrize these explicitly with categorical distributions, and optimization of the variational free energy exactly using analytical solutions resulting in a fixed-point iteration algorithm, deep reinforcement learning algorithms instead amortize these distributions with artificial neural networks, and instead optimize the variational free energy with respect to the amortised parameters (the weights of the artificial neural networks). In pure inference terms this can be seen as an E-M algorithm, with a trivial E-step (amortised inference as a forward pass through the networks), and then an iterative M-step which corresponds to a gradient step of stochastic gradient descent on the weights of the neural networks. 

The next translation is identify the value function in deep reinforcement learning with the path integral of the expected free energy over time in active inference. Therefore the derivation of the optimal policy under active inference $q(\pi) = \sigma(\int dt \mathcal{G}(\pi)_t)$ can be seen as a design choice in active inference to perform what is effectively Thompson sampling over the softmaxed value function in reinforcement learning, a choice which is often, but not necessarily made in comparable reinforcement learning algorithms \citep{osband2015bootstrapped}. Finally, the sole remaining question remains how to compute or approximate the path integral of the expected free energy, or the value function. While the discrete state-space active inference literature typically only deals with short time horizons and small state-spaces where this integral can be exhaustively computed \citep{da2020active}, or else by simply enumerating and pruning unlikely paths \citep{friston2020sophisticated}, the statespaces and time horizons in deep reinforcement learning problems are typically large enough that this approach becomes infeasible. 

The sole remaining question remains how to approximate this path integral, and here we can augment active inference with methods well used in the deep reinforcement learning community. The approach taken by model-free reinforcement learning is to utilize the iterative and recursive nature of the Bellman equation to maintain and update at all times a bootstrapped estimate of the value function \citep{kaelbling1996reinforcement,mnih2013playing}. This approach was pioneered through the temporal-difference \citep{sutton1988learning}, and Q-learning algorithms \citep{watkins1992q}, and gives rise to the model-free family of reinforcement learning algorithms. Translating this into the terms of active inference is quite straightforward. Since the expected free energy objective can be factorised into separate independent contributions for each timestep, the path integral satisfies a similar recursive Bellman-like equation. This equivalence has been recently used to prove the similarities between active inference and reinforcement learning \citep{da2020relationship}. This approach allows us to straightforwardly define Q-learning and actor-critic like active inference approaches, as was pioneered in my paper \citep{millidge2019deep}. One minor distinction is that the computation of the expected free energy contains an information gain term which necessitates a model of the states or dynamics of the world, which would not be necessary when using standard reinforcement learning approaches, but this information gain yields superior exploratory capabilities and ultimately performance.

While, in \citet{millidge_deep_2019} we explicitly included action within the generative model in the agent, so that the action prior $p(\pi)$ becomes the path integral of the expected free energy, and the variational policy posterior $q(\pi)$ becomes the independently trained policy, thus recapitulating and providing a variational inference gloss on standard actor-critic algorithms, this is fundamentally a design choice. If we instead ignore the policy prior $p(\pi)$ and define the variational policy posterior $q(\pi)$ directly in terms of the value function, we obtain an algorithm very similar to soft-Q-learning from deep reinforcement learning \citep{haarnoja2018soft}, except it optimizes a value functional of the expected free energy instead of the reward.

Conversely, we can take the approach used in deep reinforcement learning to approximate the value function at every timestep through samples of model rollouts. This is straightforward because the value function is just fundamentally the expected value of the reward across all possible trajectories under a given policy. Using model-based rollouts to approximate this is simply taking a monte-carlo approximation of an expectation where the real environmental dynamics are approximated by the model's transition dynamics. By using importance sampling on this objective, we can unsurprisingly see that the goodness of this approximation depends crucially on the match between the true and modelled dynamics.

If we are equipped with a model of the transition dynamics of the world $p(x_t | x_{t-1}, a_{t-1})$, we can approximate the path integral of the expected free energy over time in a similar way. By simulating rollouts through the transition model under a given policy, and then averaging together the path integral of the expected free energy across rollouts, we form a monte carlo estimate of the expected free energy value function. This can then be used to directly compute the posterior distribution over policies, or else can be fed into an iterative planning algorithm such as path integral control or CEM which can then be used to obtain an action plan. Using model-predictive control (replanning at every step), then allows for the creation of flexible plans for any given situation. Due to utilizing a transition model and simulated rollouts to estimate the local value function, instead of bootstrapping from previous experience, this model-based approach is substantially more sample efficient than the model-free alternative. In the \citet{tschantz2020reinforcement} paper, we took this approach and demonstrated performance comparable to or superior to standard model-based benchmarks. Additionally, as before, the exploratory properties of the expected free energy functional lead to improved performance.

Given that we thus know that active inference can relatively straightforwardly be mapped to existing algorithms in deep reinforcement learning, we now turn to the other face of the question -- whether deep reinforcement learning can learn anything from active inference. We again argue in the affirmative. Namely that active inference, through the expected free energy functional, provides superior exploratory capabilities of active inference agent which, in challenging sparse-reward tasks are necessary to obtain intelligent behaviour where random exploration is simply not sufficient. While in reinforcement learning this is a small literature on training agents with additional exploratory loss functions \citep{pathak2017curiosity,shyam_model-based_2019,still2012information,chua_deep_2018,nagabandi2019deep,klyubin2005empowerment}, active inference provides a mathematically principled and unified way of looking at this, rather than simply postulating ad-hoc additional loss functions \citep{oudeyer2009intrinsic}. Moreover, active inference also provides a theory of reinforcement learning deeply grounded in variational inference, and can thus, for instance, be straightforwardly extended to POMDP models in a way that is nontrivial for standard reinforcement learning algorithms. Finally, by proposing a unified objective of the expected free energy, active inference allows, in principle, all hyperparameters of the algorithm to be optimized directly by gradient descents against this objective, thus theoretically obviating the need for expensive hyperparamter sweeps and tuning. 

Although the close connection between deep active inference and deep reinforcement learning is now understood, and we know it is possible to scale up active inference to the level of deep reinforcement learning, there still remains much work to be done actually realizing this connection and constructing deep active inference agents, using the insights of active inference, which can compete head-to-head with the state of the art in deep reinforcement learning and, potentially, exceed it. I believe that especially as the field moves towards facing more challenging environments with sparse rewards, the exploratory drives implicitly embedded within active inference agents will become increasingly important and impactful, since many current environments possess straightforward dense rewards which provide a continuous reward gradient from any initial condition to a successful final policy. In such environments purely random exploration suffices for learning effective policies. However, such environments are not in general representative of the kind of environments that face biological organisms in the real world, and thus to model their behaviour additional exploratory instincts appear to be required. 

Furthermore, on a general note, while current work has been focused on trying to maximize the commonalities between deep active inference and deep reinforcement learning, as the goal has been to establish the connection and derive proof of principle scaled up active inference models, later work should go the other way, and try to retain the scalability of deep reinforcement learning while maximizing on what is unique about active inference. 

One intriguing possibility in this direction is to experiment with more complex distributions of rewards. While active inference can be formulating in a reward maximizing way, the real object active inference handles is the biased generative model $\tilde{p}(o,x)$, or the desire distribution $\tilde{p}(o)$. While this can be defined to be equal to reward maximization by simply defining the desire distribution to be a Boltzmann distribution over the realized rewards $\tilde{p}(o) = exp(-r(o))$ \citep{friston2012active}, this is not the only way to do it. Indeed, more complex and potentially multimodal reward distributions could be defined and optimized directly in the algorithm. In theory this could lead to more flexible behaviour or, alternatively, being able to model more complex, context-sensitive or contingent rewards naturally within the framework. There has been very little work done in this direction, and it remains an exciting avenue for future work.

Another interesting direction is to explicitly model the generative processes producing action within an inference framework. For instance, search algorithms such as Monte-Carlo-Tree-Search have been vital in the success in key reinforcement learning tasks such as playing Go and Chess \citep{silver2017mastering}, and can theoretically be written in a probabilistic generative model. Doing this would then allow them to be combined productively with all the standard tools of Bayesian and variational inference and could potentially lead to substantially more flexible algorithms for action selection which would provide extremely powerful inductive biases over standard MLP policy modules which would allow fast and very effective learning. In a similar vein, continuous-action planning algorithms, which are currently rather primitive (such as CEM \citep{rubinstein1997optimization} and path integral control \citep{kappen2007introduction}) and can only explicitly model unimodal policies, are generally quite ineffective. \citet{okada_variational_2019} has shown how many of these algorithms can be directly modelled as part of a generative model and directly used in variational inference, and they use this result to derive more effective algorithms such as multimodal CEM \citep{okada2020planet}. Further extending this work may lead to the derivation of highly effective and efficient planning algorithms for continuous control, which are currently sorely lacking and which would lead to a substantial improvement in the abilities of model-based continuous control. 

Another interesting avenue, which has begun to be explored in the literature \citep{friston2020sophisticated}, which may lead to more nuanced and effective forms of exploration, is to explicitly model, in model rollouts, the change in its own beliefs the agent expects to encounter. If this is explicitly modelled, then the agent can design exploration strategies specifically to test hypotheses and explore different strategies. In short, this kind of meta self-knowledge of the likely changes of one's own beliefs are vital for the kind of scientific and experimental thinking that often characterises humans' phenomenological experience of the planning process. Such agents would be able to intelligently consider and compute the value of information both now and the expected value of information in the future under their expected future beliefs. Basic models of this have been explored using the discrete-state-space paradigm \citep{friston2020sophisticated,hesp2020sophisticated}, however figuring out how to implement this within the deep reinforcement learning paradigm in a computationally tractable and efficient way, as well as to test its performance on tasks which require such nuanced exploration strategies remains a serious challenge and a worthy research project.

Finally the perspective of active inference -- that action and control are merely inference problems over a graphical model which includes action variables -- naturally lends itself to an understanding of different kinds of inference -- specifically amortised and iterative inference \citep{millidge2020reinforcement,kim2018semi,marino2018iterative}. Understanding how these different types of action can be combined and merged together, to inherit the strengths of both and ameliorate the weaknesses of each other, is ultimately going to be very important in designing algorithms which can initially learn rapidly from data and then slowly converge to a high asymptotic performance. There is also strong, but circumstantial evidence, that a system like this, which combines iterative and amortised inference, takes place in the brain. For instance, whenever you start learning a new skill, it takes a lot of thought and explicit mental planning, but you can learn quickly without needing an extremely large number of interactions with the environment. However, as you continue to practice, slowly your skills become habitual. They do not need mental effort and can occur almost automatically, allowing you to focus on other things while they are occurring. This distinction between conscious, effortful action and unconscious, effortless habit is precisely the distinction between iterative and amortised inference. The benefits of having such a hybrid system are obvious. It is quick to learn new skills since it can explicitly plan with them, while once a skill has been practiced many times it can be offloaded onto a computationally cheap habit system. This eliminates the necessity, in current model-based reinforcement learning systems, to undertake expensive model-predictive control and planning on every single timestep, even when the system has practiced a given contingency many many times, and also where computing the best action is actually extremely straightforward.

While we have undertaken some preliminary work in this direction, as is reviewed in this thesis, really the combination of the two to design systems with explicit planning and habit systems is just beginning and there are very many questions which remain unanswered. For instance, what is the best way to train the habitual system -- should it be trained to mimic the decisions of the explicit planner, or be completely independently trained on the reward, or both (i.e. by using the output of the planner as a regulariser of some kind)? Should the habit system and explicit planner optimize separate reward functions (for instance, should the planner be more exploratory and the habit system just optimize rewards?)? Should the habit system be used by the planner in any manner -- for instance habit value functions could be used to endstop the model rollouts of the planner to provide better local value function estimates? Should the habit policy be used to initialize the planner? How should these systems interact to produce actual output actions -- should the output be the sum of both systems? or should there be some gating mechanism which selects one or the other to produce the output? If there is such a gating mechanism, how does it work and how should it compute? How do we know when to turn off the planner and just use the habit system, if ever? The answer to all of these questions is a fascinating combination of engineering practice and machine learning theory, and the end result of getting these questions right will be flexible, robust, adaptable and sample-efficient systems which also have a high asymptotic performance with low latency and computational cost when the habit is established. Such systems could also be used to model similar computations that occur in biological brains and may shed light into the design choices available for such system and, ultimately, their neural implementations.


\section{Question 2: The Mathematical Origins of Exploration}

In Chapter 5, we delved deeply into the mathematical origins of exploratory behaviour. We saw that to obtain information-seeking exploration as a core part of the objective functional, in addition to reward maximization crucially entails minimizing a \emph{divergence} objective instead of an evidence objective. We then related this new dichotomy between divergence and evidence objectives to a wide range of currently used objectives within the reinforcement learning and theoretical neuroscience communities. The importance of this result, really, lies not in the relationship to existing methods, but what it tells us about the deep foundation of exploration. Put simply, we see that extrinsic exploratory drives emerge from trying to match rather than maximize. Matching tries to maintain the complexity of the inputs, so that given a complex desire distribution, agents are driven to stabilize a similarly complex future. Conversely, maximizing implicitly tries to simplify the inputs, ideally a maximizing agent would collapse all future inputs to a dirac delta around the future reward. It is only the extent to which there is uncertainty in the world, or in the reward function, or a lack of controllability in the world which prevents this full maximization. This difference is what gives rise to intrinsic exploratory behaviour in the divergence minimization case, and to effectively anti-exploratory, information-minimizing behaviour in the evidence, reward, or utility maximizing case. We additionally see that reward maximizing agents do not have, and cannot have, any intrinsic exploratory drives, since the very nature of their objective compels them to minimize information gain. Information and learning, to them, is a cost which must be borne, and not a reward to be pursued for its own sake.

This additional information gain term is very important, because it drives agents which optimize it to explore and seek out new contingencies, and to find and update upon resolvable uncertainty in their world. This means that agents which have an expressive desire distribution, will tend to learn faster and better world models, as well as pursue more exploratory policies, which in the long run lead to higher performance than purely reward-maximizing agents, even when judged on rewards alone. This has been investigated by ourselves in Chapter 4, as well as under many other approaches in the literature. What is most interesting here is that our understanding of these objectives as \emph{divergence} objectives provides a precise mathematical characterisation of what these objectives are implicitly doing.

It is important to note that it is possible to derive some form of information gain exploration directly from reward maximization -- in the form of explicitly computing and calculating with the \emph{value of information} \citep{still2012information,schmidhuber2007simple,osband2019deep,tishby2011information}, where we can operationalize the value of information purely in terms of reward as the additional amount of reward expected given better policies as the result of obtaining and integrating the information into your world and policy models. While this value of information computation is theoretically optimal given a reward maximization objective, in practice it is intractable to compute exactly, and there has been little investigation in the literature as to direct approximations of this term. However, there are some heuristic approaches which seek to approximate it, although it is not clear how well. For instance, \citet{osband2019deep} argue that using `optimistic value functions' which automatically up-weight unknown contingencies can lead to a good approximation of the value of information, since in practice, the agent will automatically explore until it has diminished its optimistic bias to the extent that the bias for all contingencies is lower than the current best option. This approach, under the names of the upper-confidence bound is widely used in the multi-armed bandit literature \citep{garivier2011upper} and additionally has been used to great effect in Monte-Carlo Tree Search algorithms \citep{kocsis2006bandit}, and has been investigated to some degree within deep reinforcement learning \citep{silver2017mastering}. Another approach, known as Thompson sampling \citep{russo2016information}, explicitly computes or approximates the posterior distribution over actions given the current history of observations and rewards, and then achieves some degree of exploration by sampling from this posterior -- the idea being that in uncertain regions, the posterior distribution should be fairly uniform, and thus provides effectively random exploration, while when the posterior is sharp then it is likely that the true optimum has been found and thus exploration is unnecessary and costly. 

An important challenge is that inference based approaches such as control as inference do not naturally compute any analogue of the value of information. This is because, ultimately, these approaches take a mean-field factorisation across time and split their objective up across time-steps. This means that information can only flow through time through the transition model, and thus the agent cannot model any kind of learning in the future, where information it may or may not discover in the future leads it to change its model, leading it to perform better (or worse) in the future. Due to this limitation, which is ultimately applied for reasons of computational tractability, approaches like control as inference do not compute any kind of value of information across time-steps. This limitation also applies to the information-seeking methods we discuss which arise from divergence functionals. If these functionals are also mean-field factorized, then agents only seek to maximize the information-gain in the current time-step. Extending these methods by relaxing the temporal mean-field assumptions will likely yield more effective and nuanced forms of exploration, which can induce consistent exploratory behaviour across multiple timesteps and thus handle more complex contingencies. However, designing effective and mathematically tractable algorithms which can do this largely remains an avenue for future work.

However, this information-maximizing exploration with divergence functionals is not done explicitly to gain any kind of future reward. Instead, agents optimizing divergence objectives treat optimizing information gain as an intrinsic good. This is what allows an information gain objective to arise even though the objective satisfies the same mean field assumptions as the control as inference objective. However, this means that to some extent divergence minimizing agents will continue to explore beyond the point which is strictly necessary for reward maximization. This means that, in effect, from the perspective of a purely reward maximizing agent, divergence minimization is just an additional exploratory heuristic by which to approximate the value of information terms within a mean-field formulation. However, in general, it has been empirically found that the information seeking exploration in a mean-field fashion, while not strictly the value of information, gives a good approximation in general and will lead to good performance especially in high dimensional, sparse environments. However, because it explicitly trades off a reward maximizing and an information-seeking objective, it will tend to over-explore relative to pure reward-maximizing value of information computation, by exploring regions with much resolvable uncertainty but relatively little reward, and will continue exploring even when it is likely (but not certain) that the optimal solution has been found. However, as long as all uncertainty in the environment is resolvable, then the divergence objective will eventually converge to the reward maximization objective since the information gain term will eventually become negligible once the agent possesses a very good and accurate world model.

An interesting direction for future work will lie in relaxing the mean field assumptions which currently underpin all of these functionals. While it is likely that a full relaxation will be intractable, there are many intermediate relaxations which have been proposed in the general variational inference literature which could prove highly fruitful within the control task. For instance, the Bethe free energy and related objectives \citep{yedidia2001generalized,pearl2014probabilistic,schwobel2018active}, allow for temporal pairwise correlations to be explicitly considered. Moreover, there is a highly general family of `region graph' approximations \citep{yedidia2005constructing,yedidia2011message} which have been developed within the variational inference literature, and which allow for more complex interactions to be modelled within a relatively tractable computational framework and which have been found to improve inference performance. If there is a way to compute successively better Bayesian approximations to things like the value of information, or to relax the temporal mean-field approximations made in contemporary divergence and evidence objectives for control, it will likely come from a thorough mathematical and experimental investigation of these more advanced and accurate approximation techniques. Understanding how the information gain functionals from divergence objectives function and change as the temporal mean-field approximation is relaxed is especially interesting, since preliminary investigations, even within the mean field paradigm, show that when expressly writing out the objective in terms of entire trajectories, terms similar to empowerment \citep{klyubin2005empowerment} and filtering information gain (backwards in time) result. 

Related to the relaxation of the temporal mean-field approximation, there also needs to be much work allowing agents to explicitly model changes to their own beliefs in the future. This is necessary to truly compute realistic information-seeking objectives when utilizing multi-step planning algorithms, since currently all information gain is computed with respect to the agent's current beliefs, which will not necessarily hold in the future if and when it actually eventually reaches this information. Such a process would enable an agent to understand how various kinds of information would change its own beliefs and policies, and thus be able to plan for multiple sequential `realizations'. Human planning is certainly capable of such introspective capabilities, where we can, for instance, decide to seek out and investigate certain phenomenon in order to be able to understand and better seek out information about another task, and so on. Some fascinating recent work has begun to explore these sorts of metacognitive abilities within the active inference framework \citep{friston2020sophisticated}, but only within a discrete state space and nonscalable task. The true issue with such approaches will be their inherent computational difficulty, since on a naive approach they will require the agent to simulate its own belief updates, requiring it to store and introspect upon a copy of its own models and inference procedures. However, since such metacognitive abilities are likely crucial to effective long range planning, an important strand of future work will be designing approximations and objectives which can accomplish this in a computationally tractable manner.

Additionally, the divergence vs evidence framework presents a new class of objectives which are, in theory, distinct from the usual paradigm of reward or utility maximization. Here, instead we simply seek to minimize divergence to a complex reward or desire distribution. In theory, the idea of having a complex, multimodal and potentially non-scalar desire \emph{distribution} instead of a simply scalar reward function to maximize is that in theory it allows for more complex notions of goals or desires to be implemented and optimized by agents. Specifically, it allows for \emph{vector-valued}, and \emph{multimodal} goals which are not well handled within the standard reward-maximization framework, but which can be straightforwardly handled within our formulation of a desire distribution by both evidence and divergence objectives. While current work has mostly focused on demonstrating the equivalence of reward-maximization, and the desire distribution under certain conditions (the desire distribution being a Boltzmann distribution of the reward), and thus showing that the probabilistic case is a strict generalization of the scalar deterministic case, the real interest of the probabilistic representation is precisely how it can differ from simple reward-maximization. Much work remains to be done to understand the possibilities for more flexible and expressive goal or value representation which is unlocked by this more general formalism, and how it can be leveraged to design artificial systems which perform more capably in practice.

Finally, the idea of divergence minimization also has extremely close links with Markov-Chain-Monte-Carlo inference procedures, where it has recently been realized that much of this paradigm can be expressed within a simple framework of stochastic dynamical systems theory, whereby all the various samplers can be interpreted as implementing a certain stochastic differential equation minimize explicitly performs a gradient descent on a divergence objective \citep{ma2015complete}, with different algorithms in the literature simply specifying different noise terms and solenoidal flows \citep{yuan2017sde}. Beyond this, the idea of divergence minimization can also intriguingly be linked to recent advances in stochastic non-equilibrium thermodynamics \citep{seifert2012stochastic}, which has developed ways to translate classic thermodynamic notions of entropy and entropy production from properties of large ensembles to properties of individual statistical trajectories \citep{esposito2010three1}. A crucial result in this new formalism of stochastic dynamics is that any system with positive entropy production can be construed as minimizing a divergence between its current state and its ultimate steady state density \citep{esposito2010three1} -- and can thus be interpreted as performing a form of direct divergence minimization -- thus potentially implying that this objective may in some sense be a more natural one for systems and agents to perform than pure evidence maximization, and secondly that the laws of thermodynamics themselves may implicitly require information-maximizing behaviour from disspative non-equilibrium systems. While these links currently remain speculative, and further investigation will likely discover greater nuance and require some qualification of these claims, in my opinion there is significant potential here to link discoveries in stochastic thermodynamics to help us build a fully general picture of the necessary nature of exploratory behaviours in systems evincing the classic action-perception loop.

\section{Question 3: Credit Assignment in the Brain}

By showing that predictive coding can approximate backpropagation along arbitrary computation graphs, instead of just MLPs, we have specifically turned predictive coding from a direct model of brain function or perception, into a learning algorithm which can be applied to arbitrary architectures. This dual perspective, where predictive coding is both a generic learning algorithm, as well as specifically a model of learning and perceptual inference in the brain is most interesting and, as far as I am aware, is unique to predictive coding. Specifically, casting predictive coding as a learning algorithm makes clear several interesting correspondences between inference procedures and learning. Specifically, that we can derive a learning algorithm on a computational graph by trying to infer the values of the nodes in the graph. Predictive coding additionally provides for a straightforward extension to backprop in the form of precisions, which allow for the learnable up or down weighting of certain gradient signals depending on the intrinsic noise of their generating process. Such a system, while not particularly useful in the standard machine learning paradigm of independent, identically distributed datasets, may prove extremely important for learning with more ecologically valid sensory streams which contain various amounts of noise and distractor information which should not be learnt from. This perspective of learning and credit assignment as a kind of inference also immediately lends itself to further applications and extensions beyond just precision. For instance, the effect of different generative models other than Gaussian remain to be determined, as well as potentially different optimization procedures and variational functionals to be optimized. In general, this perspective allows the highly developed machinery of inference in graphical models \citep{pearl2014probabilistic,ghahramani2001propagation,beal2003variational,yedidia2011message} to be deployed to improve credit assignment and optimization algorithms. This area, I believe, is an exciting and potentially highly impactful one for future work since the impact of improving either credit assignment or optimization processes, which are at the heart of all of modern machine learning, will necessarily be substantial.

Furthermore, by demonstrating that many of the biological implausibilities in the predictive coding scheme can be relaxed \citep{millidge2020relaxing}, we, for the first time, demonstrate a biologically plausible local approximation to backprop which does not suffer from either issues of locality or issues of weight transport. Furthermore, we develop a novel and much simplified algorithm -- Activation Relaxation -- which possesses relatively straightforward, local, and elegant update rules when compared with predictive coding which succeeds in approximating the backpropagation of error algorithm to arbitrary accuracy given enough iteration steps. Moreover, we have shown that the same relaxations which work with predictive coding, also work with the activation relaxation algorithm, thus demonstrating the robustness and efficacy both of the AR algorithm and of the methods of relaxation utilized. Crucially, if we look at the final, relaxed AR update rule (Equation \ref{fully_relaxed_AR_update}, we see that the change in activities for a layer only requires the current activation values of the layer above, mapped through the backwards weights which are learnt independently of the forward weights, be subtracted from the current activation of the layer. This is sufficient, over a number of iterations, to allow the activations of each layer of the network to converge to the gradients of backprop. Then, once this is achieved, the weights can be updated. 

While these algorithms have considerable advantages -- they exactly approximate backprop given enough iterations, they require only biologically plausible local update rules, and they are simple and potentially straightforward to implement in neural circuitry, they also have substantial disadvantages. I believe these disadvantages are worth discussing in some depth since they provide a precise specification of the areas for improvement in current algorithms. The fundamental disadvantage of iterative schemes like this is their iterative nature. Specifically, they require separate phases of operation -- a forward phase which is equivalent to a feedforward pass through the network, and a backwards iterative phase of multiple dynamical iterations. A significant issue is that it is unclear whether such phases can realistically exist within the brain. While there are evidence for different oscillatory frequency bands in the brain \citep{buzsaki2006rhythms}, and even in superficial vs deep cortical layers \citep{bastos2020layer,bastos2015visual}, it is unclear whether these rhythms do, or can, coordinate separate feedforward and iterative phases. Such a scheme, if implemented in the brain, would form a kind of clock, only allowing feedforward information to be processed in short bursts in between the iterative phases. While not impossible, this seems at odds with our current understanding of the brain where feedforward and feedback inputs are combined together in real time. Another very straightforward problem is simply the number of iterations these schemes require. The brain cannot wait for tens or hundreds of iterations until convergence, even under generous assumptions about rhythmic activity implementing the phases, while these schemes often require a substantial amount of iterations to converge to nearly exactly the backprop gradients. While this could be ameliorated somewhat with high learning rates, and settling for less than exact convergence to the backprop gradients, it has not yet been extensively investigated whether the algorithms are even stable under such conditions. Understanding and optimizing the number of iterations and various parameters like the learning rate is still an open area of research. Unlike backprop with deep neural networks, where all hyperparameters have been effectively extensively tuned in the literature over a decade of experimentation, good hyperparameter settings for these alternative algorithms have barely been explored at all, and their empirical limits of performance at scale have largely yet to be determined. A final serious difficulty with such approaches is that to match backprop they require some level of nonlocality in time, where information from the feedforward pass is stored and then utilized throughout or at the end of the backwards pass. This storage of information is fundamentally necessary because the backprop gradients depend only on the state of the network in the feedforward pass. If the state changes due to the iterative algorithm, then the old information from the forward pass must be stored somehow to maintain convergence to backprop on the forward pass. This manifests itself in predictive coding as the fixed-prediction assumption, which explicitly assumes that the values of the forward pass are stored. In the AR algorithm, it manifests in the necessity to store the original value of the activity neurons to use to update the weights after the backwards pass is complete. Due to this storage, the actual update equations become nonlocal in time. This shortcoming could potentially be addressed in two ways. Firstly, it might be possible to store the information from the forward pass, for instance in local recurrent units which are insulated from the activity changes in the dynamical iterations, or secondly, if the number of iterations is short enough, such information could be persisted simply through multi-step recurrent connectivity. Nevertheless, even if this could be done, the circuitry to align and ensure the correct time of arrival of all the necessary signals could be quite complex.

In this field, it is important to reflect deeply upon the role and utility of simplified models of neural dynamics. Almost all work in this area operates with very simple models of rate-coded integrate and fire neurons, typically often in a temporally static `instantaneous' computation graph. Biological plausibility within this model, while a somewhat vague concept, is often defined by conditions such as only local connectivity, or that connectivity must be additionally Hebbian, and that neural connectivity cannot be too precise, and that information cannot be directly transmitted backwards along axons. However, this definition necessarily ignores some important aspects of reality. Obviously the brain uses spiking neurons and must also achieve credit assignment through time, but additionally there are even questions about the simplification of neural architecture. For instance, typically such models implicitly assume a multilayer perceptron (MLP) style architecture as just a stack of fully connected layers, however each region of the cortex has an intricate 6-layer structure, and it is not clear whether each such layer in the cortex should be considered as a single layer of the MLP, or only the cortical region. If the former, then the different properties and neurophysiology of each cortical layer is not modelled. If the latter, then it is not at all clear to what extent the entire cortical region \emph{can} be modelled as a simple fully connected layer. An additional interesting neurophysiological fact which is rarely explicitly modelled is the predominance of cortical columns in the visual cortical regions. These columns do not at all correspond to fully connected layers and it is not yet fully clear what their computational role is. A straightforward hypothesis is that they are the brain's way of implementing a local receptive field operation, like convolution in convolutional neural networks, but without the advantage of shared weights across space which is core to the generalization capabilities of the CNN \citep{hawkins2007intelligence}. 

The question remains, however, how applicable are such simplified rate-coded models to the full complexity of credit assignment in spiking networks through time. The hope, ideally, is that by and by large important computational primitives and algorithms which have been developed for biologically plausible credit assignment in rate-coded neural networks remain functional, or only require minor adaptation, to work in a spiking context. This is a strong possibility, but it could also be completely false, and that the brain implements an algorithm which relies heavily on the unique properties of spiking networks for its credit assignment capabilities. Ultimately, before we fully understand the mechanisms of credit assignment in the brain, it will be hard to fully assess the degree to which work on rate-coded models will generalize. There has been some preliminary successes though. For instance, the surrogate gradient technique shows that with only minor tweaking (defining a surrogate gradient to avoid the nondifferentiable threshold spiking function), backprop through time (BPTT) can be straightforwardly used to train spiking neural networks for complex tasks \citep{zenke2018superspike,neftci2019surrogate}. However, these methods currently utilize the biologically implausible BPTT algorithm and it is unclear to what extent biologically plausible alternatives to BPTT can be straightforwardly applied in such a manner. To answer such a research question would be an important and timely research agenda which would substantially advance our understanding of the generalizability of such models.

Fundamentally, the core challenge is that of time. Most models (with some exceptions \citep{bellec2020solution,schiess2016somato}) focus only on backpropagation and credit assignment through space (i.e. layers of a neural network architecture) and not through time. However, time is an inextricable component of the computation in the brain. Fundamentally, perception in the brain is not about handling static $i.i.d$ datasets, but rather \emph{filtering} on constantly changing sensory streams of information. Moreover, credit assignment through time is in some sense a substantially harder problem, in that the key information necessary at each step disappears, while when backproapgating through space the information is always present somewhere in the graph. In the case of BPTT, all the intermediate activations at each timestep are stored and then replayed in backwards sequence once the sequence has ended. However, such an acausal solution is clearly not suitable for computation in the brain. There are alternatives to BPTT which do not require explicitly repeating computations backwards through time, but allow for online integration of gradients. The key such algorithm is the RTRL algorithm which maintains a Jacobian of values at each timestep and iteratively updates them at every timestep. In algorithmic terms, RTRL corresponds to forward-model automatic differentiation while BPTT corresponds to reverse-mode. Unfortunately, however, RTRL is substantially more expensive to implement on digital computers, scaling with $O(n^4)$ where N is the number of parameters, rather than $O(n^2T)$ for BPTT, where T is the time horizon. One key advantage of RTRL however is that unlike BPTT it is not bound by sequence length. While BPTT must choose some point to stop and then backpropagate all the gradients and then truncate gradients from after time T, RTRL can operate on sequences with indefinite length without issues, although credit is slowly diluted away through time. However, RTRL is also likely not neurally plausible and it is not clear how to implement RTRL in systems with multiple recurrent layers interacting without a blowup in the number of learning rules required. My personal hunch is that extending current models of local, biologically plausible credit assignment to spiking neural networks will not be especially challenging, although they might not be the method that the brain uses. However, I believe that credit assignment through time is a fundamentally different and more challenging problem than credit assignment through space, and that ultimately, to solve the problem of credit assignment in the brain we must grapple head on with the problem of local credit assignment through time. 

There are already some methods existing in the literature which accomplish this, specifically eligibility-prop \citep{bellec2020solution} proposes eligibility traces to compute the RTRL algorithm in a local fashion. However the method currently only works for a single recurrent layer while the brain is deep both in time and in space, and that this depth requires new algorithms, or the combination of existing algorithms for backprop in space with those for backprop in time. It is also unclear to what extent the brain computes the full credit assignment mandated by RTRL or else some sparse approximation therein, perhaps aided by the natural sparsity properties of spiking neural networks. 

An additional consideration when thinking about credit assignment in the brain is the question of feedforward vs feedback processing \citep{kriegeskorte2015deep}. While the BPTT is only designed for backpropagating through immediately recurrent feedforward neural networks, the brain actually contains a multitude of long-range recurrent loops mediated by top-down feedback connectivity \citep{felleman1991distributed,grill2004human}. Understanding the properties of these and whether they are used for credit assignment, or whether the brain computes credit backwards through these top-down connections is also crucial for understanding the full picture of credit assignment in the brain. An additional, and almost entirely unanswered question is the role of long term memory in credit assignment. Current methods, including BPTT, typically use some temporal cut-off to stop considering gradient information beyond some time-horizon, and a key flaw in naive recurrent architectures is that information is slowly lost over time \citep{ollivier2015training,hochreiter1997long}. The key innovation in LSTM units is that they contain an explicit forget/remember gate which allows for memories to be stored potentially indefinitely \citep{hochreiter1997long}. There are additionally various modifications for recurrent RNNs which help ameliorate this problem as well \citep{ollivier2015training}. Since (we assume) the brain is equipped with a fairly standard recurrent architecture, the architectural or learning-rule modifications that enable it to avoid this problem and to successfully store and utilize even long term credit assignment remains to be explored and is a very important and fundamental question in understanding credit assignment and learning in the brain. One hypothesis is that synapses in the brain may store a temporal hierarchy of eligibility traces which retain information over progressively longer timescales, thus allowing them to act on information which has been accumulated even in the relatively distant past. However, figuring out the actual specific operation of such a system remains an open research challenge.

A further interesting question, raised by the recent successes of instantaneous feedforward architectures such as transformers in processing sequential data such as natural language text over recurrent architectures such as LSTMs is to what extent recurrence is actually a useful computational primitive for handling sequence data as opposed to one the brain may be forced into due to its inherent computational constraints and need for online processing instead of batch processing at the end of a sequence, as in transformers. A key advantage of attention, as implemented in transformers \citep{vaswani2017attention}, is that it enables arbitrary time-to-time modulation, rather than in recurrent architectures where the immediate past inputs are combined with the present ones to predict the future. In this way, transformers can handle data in a fundamentally acausal manner, with accompanying computational advantages. It remains to be seen whether attention like mechanisms can be implemented in a recurrent way, whether recurrent architectures can be successfully scaled to the level that transformers have done, or whether they remain on an inferior scaling curve \citep{kaplan2020scaling}, and the precise mechanism by which similar sequence computations are implemented in the brain. The key lesson of attention is that recurrence is not the only way to handle intrinsically sequential inputs. It remains to be seen whether other non-recurrent mechanisms are implemented in the brain.

Importantly, although this line of work has focused on determining whether the backpropagation of error algorithm can be implemented in the brain, which is inspired by the impressive success of modern machine learning, which is based on this algorithm, it is not entirely certain that the brain achieves credit assignment through backpropagation at all. There are several alternative methods which are worth discussing in some detail. For instance, it is also possible that the brain may be implementing some more advanced kind of learning algorithm than stochastic gradient descent. \citet{meulemans2020theoretical} showed that target propagation implements an approximate version of a second order gradient descent scheme -- known as Gauss-Newton optimization. Similarly there are other optimization methods, such as conjugate gradients, or coordinate ascent, or fixed-point iteration, as well as a variety of probabilistic message passing schemes which do not require explicit gradients to be computed \citep{yedidia2011message,parr2019neuronal}. Moreover, it is possible that the brain, while needing to compute gradients, does not compute by the backpropagation of error algorithm. For instance, it is possible to compute gradients by finite differences, and although these finite differences perform strictly worse than automatic differentiation techniques computationally, they are very simple to implement in practice. For instance, the brain could easily contain circuits which could compute time derivatives of a constantly varying temporal stream. Then, once we have the time derivatives of two variables, it is possible to directly compute their gradient by
$\frac{dx}{dy} = \frac{dx}{dt} / \frac{dy}{dt}$. Such a circuit would only need local temporal finite differences as well as a divisive feedback connection to combine the two time derivatives, well within the possibilities afforded by known neurophysiological constraints. A further option would be that the brain simply may not compute with derivatives at all, all the while optimizing some objective function. There are many `black box' optimization methods which do not require gradients of the objective. For instance, genetic algorithms \citep{salimans2017evolution}, or other brute-force-esque algorithms may instead be implemented, and indeed it has been shown that in some cases genetic algorithms, when sufficiently scaled are able to compete with backprop on some optimization tasks \citep{salimans2017evolution,such2017deep}. Another possibility, is that the brain could learn solely by global rewards broadcast to all neurons, where learning effectively takes place via the policy gradient theorem \citep{roelfsema2005attention}. There is some circumstantial neurophysiological evidence in favour of this -- specifically the well known role of dopamine as a spur to synaptic plasticity \citep{dayan2009goal,dayan2008decision}, as well as the fact that many cortical pyramidal cells receive global dopaminergic inputs from subcortical regions. There have also been a variety of models \citep{roelfsema2005attention,pozzi2018biologically} proposed of this kind of global reward-driven learning. However, this type of learning suffers from a severe intrinsic flaw that the gradients it estimates for each parameter have extremely high variance. This is because each neuron is only provided with a global reward signal, which is ultimately caused by the interaction of an extremely large number of other neurons. Thus, averaging out all the noise introduced by all the other neurons in the brain requires a very large statistical sample, thus effectively leading to extremely high variance gradients and slow learning, which scales increasingly poorly with network size. This is precisely why backprop is such a useful algorithm, since it provides precise feedback to each neuron about the loss, thus meaning that the only source of noise is minibatch noise rather than intrinsic noise due to the activities of other neurons. This means that backprop computed gradients have much lower variance and can lead to much faster learning. Nevertheless, it remains inarguable that pyramidal cells are generally innervated by dopaminergic inputs and that dopamine, which is released when there is a reward prediction error in the basal ganglia \citep{schultz1998predictive,schultz1998reward,dayan2009goal} can strongly modulate learning. This may imply that there are effectively two learning systems on top of one another in the brain. The first, potentially older, is the global slow dopaminergic system, while the second, entirely cortically based uses precise vector-feedback with some backpropagation like algorithm. The global reward signals, then can potentially modulate various aspects of learning so that contingencies with high reward prediction error are especially salient and may induce larger changes than those without \citep{daw2006cortical,roelfsema2005attention}.

Finally, it is always possible that we have been misled by the contemporary successes of machine learning, and that the core formulation where we perceive the brain implicitly or explicitly optimizing some objective is simply wrong, and that the brain cannot be described in such a way at all. While this is a very general formulation of perception and learning, with support from both machine learning and statistics and control theory in engineering, it nevertheless could not be a productive framework in which to think about the function of the brain. Such a scenario would arise, for instance, if the brain was merely a grab-bag of various heuristics and reflexes, implicitly tuned over the course of evolution, without any serious potential for learning. While this may be true of the brains of some simple animals, it is clearly not for humans and other creatures with complex, learnt cognition. Nevertheless, if this is somehow the case, the question will then become what is a productive mathematical framework in which to think about what the brain is doing, if not in the language of probabilistic models and objective functions. It may be that the answer to this question, if it must be posed, is more interesting and productive than discovering that the brain was simply doing backprop all along. Nevertheless, it is extremely unclear at present which of these possibilities is true. My opinion, given the mathematical elegance and generality, as well as the empirical successes of the objective function viewpoint, is that it is a valuable and most likely correct framework for understanding the operation of the brain. However, it is always worth noting that this is purely a speculative hunch, and there remains little non-circumstantial evidence one way or another.

\section{Closing Thoughts}

This thesis is titled `\emph{Applications of the Free Energy Principle for Machine Learning and Neuroscience}', and throughout the thesis we have endeavoured to demonstrate how to adapt and extend methods from the free energy principle and its primary process theories -- active inference and predictive coding -- to make advances in deep reinforcement learning, and in neuroscientific theories of perception and credit assignment in the brain. Interestingly, the pattern of progress in this thesis is that, in parallel, between the neuroscience and the machine learning, is that first we simply try to adapt and extend the methods of the free energy principle to test their capabilities against other existing methods from the core literatures of these subjects, and then strive to show how process theories like active inference and predictive coding can extend and advance upon the current state of the art. Then, this process inevitably reveals new and interesting questions by itself, which are somewhat separate from the free energy principle and its process theories. These questions are firstly the mathematical origin of exploration (which emerges from considering the nature of the expected free energy objective in active inference), and secondly credit assignment in the brain, which emerges from considering how predictive coding relates to backpropgation of error. In the second set of chapters (5 and 6) we have then tried to address these further questions, using methods \emph{inspired} by the free energy principle, but not necessarily directly deriving from it. We believe that in many ways this reflects the theoretical fertility and utility of the FEP as an abstract principle, that it allows the postulation and exploration of deeper questions than would be possible without it. Indeed, this theoretical fertility may be one of the primary means of judging the utility of the FEP since, as we discussed in Chapter 2, the FEP is technically non-falsifiable and can be considered more of a mathematical principle, or perspective, rather than a theory. If this is the case, then the work in this thesis provides support to the contention that the FEP provides a useful and fruitful perspective to understanding a variety of questions both machine learning and neuroscience.

The phenomenological process of intellectual understanding is an interesting one. At first you appear to spend ages groping around in the dark, hitting various unknown objects in your way, but slowly gathering a picture of the obstacles in your path. Then, at long last, you eventually stumble your way to a light-switch and the whole vista is revealed. What were once unknown lurking obstacles are transformed into precisely specified, and tractable, objects, whose relation to one another can be seen at first glance. In the light, everything is at once clearer, but also \emph{smaller}. Questions and dilemmas which seems huge and irreducibly complex are revealed to be straightforward, even trivial, in the light of understanding. So much so that it is often easy to forget, looking back, how these questions appeared when the answer was unknown. From a personal perspective, and one I have hoped to share in this thesis, I believe I have managed to obtain and present a clear understanding of two topics which were not clear in the literature before my PhD -- whether active inference can be successfully scaled up to compete with modern reinforcement learning methods (and the relationships between the two theories), and the mathematical origins of the exploration term in the expected free energy, and its relationship to more standard functionals such as the variational free energy for perception. I have additionally contributed to the theory and implementation of predictive coding models, as well as algorithms for biologically plausible backprop in the brain. I feel, however, that while I have uncovered certain key aspects and facets of the problem of credit assignment in the brain, which were previously shrouded, I have not yet reached the point where all mystery falls away and the light pours in. Similarly, I hope that the work in this thesis has helped you, the reader, understand some things at least a little more clearly.

To conclude, we offer some ideas of for the future development of the ideas worked out in this thesis. We believe it is clear, we have shown, that active inference can be productively related and merged with the large and extremely powerful set of deep reinforcement learning agents to enable general and flexible learning based algorithms which can succeed in challenging tasks, even those requiring a considerably degree of exploration. Further progress in this field, from the perspective of active inference, should move beyond merely scaling up, and instead focus on the unique insights and ideas that active inference brings to the table. Examples of this include its distinct and exploratory objective functionals such as the Expected Free Energy, or the free energy of the Expected Future. Another potentially interesting avenue lies in active inference's more flexible consideration of reward as a specific prior \emph{distribution}, rather than a scalar value. This could enable more flexible behaviour through better reward speicfication, and include the ability to learn flexible reward distributions on the fly, effectively performing reward shaping in a semi-autonomous manner. A final avenue for exploration is the refinement of the specific generative models used in control agents. A key tenet of active inference is the idea of deriving powerful behaviours from inference on detailed and flexible generative models of the dynamics of the world. While in this thesis, we have focused primarily on simply approximating and parametrizing the distributions in the generative model with deep neural networks, a more refined approach might be to investigate whether the world can be factorized in a tractable manner, and design generative models to explicitly exploit these factorizations to allow for more tractable and efficient inference -- effectively giving the agent just the right inductive biases about the world to subserve its control objectives. Secondly, for the question of credit assignment in the brain, I believe that the key advances will come in understanding the temporal component of credit assignment, and this requires deeply understanding the fundamental computations of the brain as performing dynamical algorithms such as filtering and smoothing rather than merely static Bayesian inference. This line of research would focus attention both on how the brain can achieve backpropagation through time as well as through space. There are also extremely interesting links here with predictive coding, where dynamical approaches using generalized coordinates remain underexplored, and it may be that by further developing the explicitly Bayesian approaches to filtering provided by the dynamical formulations of predictive coding, we can better characterise and understand the temporal nature of the brain's computations. The work in this thesis relating predictive coding and Kalman filtering is only the beginning, there needs to be much work done scaling and precisely characterising the performance of dynamical predictive coding algorithms, understanding whether such recurrent predictive coding networks can be utilized to perform some form of temporal credit assignment, as static predictive coding networks can.
